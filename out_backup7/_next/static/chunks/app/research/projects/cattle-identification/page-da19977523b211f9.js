(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[4542],{546:(e,t,i)=>{"use strict";i.r(t),i.d(t,{default:()=>u});var n=i(5155),a=i(6766),r=i(6874),s=i.n(r),o=i(7168),l=i(3059);let c={ja:{title:"3Dカメラによる歩行中の牛の個体識別",subtitle:"距離情報を活用した高精度な牛の個体識別システム",backToProjects:"研究プロジェクト一覧に戻る",overviewTitle:"3Dカメラによる牛の個体識別",overviewText:"牛の個体識別は、個体の成長、健康管理、行動パターンなどを個別に追跡・管理するために重要です。本研究では、距離情報を取得できる3Dカメラを用いることで、毛色や模様に依存しない高精度な牛の個体識別システムを構築しています。",backgroundTitle:"実験背景・目的",backgroundText1:"牛の個体識別によって個体の成長、健康管理、行動パターンなどを個別に追跡・管理できます。また、異常な行動や疾病の早期検出が可能となります。しかし、通常のカメラでは毛色や模様が一様な牛の識別が困難です。そこで、距離情報を取得できる3Dカメラを用いることで、牛の個体識別を行うことを目的としています。",backgroundText2:"従来のRGBカメラを用いた個体識別では、照明条件の変化や牛の体表の汚れなどによって識別精度が低下するという課題がありました。3Dカメラを用いることで、これらの環境要因に影響されにくい、より安定した個体識別システムの構築を目指しています。",environmentTitle:"実験環境",environmentText1:"農工連携のもと、宮崎大学住吉フィールドから収集したデータを使用して実験を行いました。上の図に示すように、牛舎の上部に3Dカメラを設置し、上方から牛を撮影しています。これにより、牛の背中の形状や高さなどの3次元的特徴を捉えることが可能となりました。",environmentText2:"3Dカメラから取得した深度情報は、CSVファイルとして保存され、後続の処理で活用されます。この方法により、照明条件や牛の毛色に左右されない安定したデータ収集が実現しています。",methodTitle:"提案手法",methodText1:"個体識別のフローチャートを示しています。距離情報が入ったCSVファイルから牛領域の検出を行い、得られた牛領域から特徴量を抽出し、機械学習を用いて個体識別を行います。",methodText2:"具体的な処理フローは以下の通りです：",methodList:["3Dカメラから取得した深度情報の前処理（ノイズ除去、正規化）","背景と牛領域の分離（閾値処理、領域分割）","牛の3次元形状に関する特徴量の抽出（背中のライン、肩の高さ、腰の形状など）","機械学習アルゴリズムによる個体識別モデルの構築","リアルタイム識別システムの実装"],resultsTitle:"実験結果",resultsText1:"4つの機械学習分類器（A〜D）を用いた個体識別の精度を比較した結果です。分類器Dでは正答率が95.0%となり、高い精度で個体識別が可能であることが確認されました。",resultsText2:"特に、照明条件の変化や牛の体表の汚れなどの環境要因に影響されにくく、安定した識別性能を示しました。また、歩行中の牛に対しても高い精度で識別が可能であり、実用性の高いシステムであることが確認されました。",futureTitle:"今後の展望",futureText1:"本研究の成果を基に、より多様な環境や牛種に対応できるよう、データセットの拡充とモデルの改良を進めていきます。また、個体識別だけでなく、歩行パターンの分析による跛行検知や、体型変化の追跡によるBCS評価など、他の健康指標との統合も視野に入れ、総合的な牛の健康管理システムの開発を目指しています。",futureText2:"さらに、エッジコンピューティング技術を活用したリアルタイム処理システムの構築や、クラウドシステムとの連携による大規模牧場向けの統合管理システムの開発も検討しています。これにより、日本の酪農業の生産性向上と持続可能な発展に貢献することを目指しています。",futureText3:"本研究は、SDGsの目標2（飢餓をゼロに）、目標8（働きがいも経済成長も）、目標9（産業と技術革新の基盤をつくろう）に貢献することを目指しています。"},en:{title:"Individual Cattle Identification Using 3D Camera During Walking",subtitle:"High-Precision Cattle Identification System Utilizing Distance Information",backToProjects:"Back to Research Projects",overviewTitle:"Cattle Identification Using 3D Camera",overviewText:"Cattle identification is important for individually tracking and managing growth, health, and behavioral patterns of each animal. In this research, we are building a high-precision cattle identification system that does not depend on coat color or patterns by using a 3D camera that can acquire distance information.",backgroundTitle:"Experimental Background & Objectives",backgroundText1:"Cattle identification allows for individual tracking and management of growth, health, and behavioral patterns. It also enables early detection of abnormal behavior and diseases. However, identification of cattle with uniform coat colors and patterns is difficult with conventional cameras. Therefore, our objective is to perform cattle identification using a 3D camera that can acquire distance information.",backgroundText2:"Conventional individual identification using RGB cameras had issues with decreased identification accuracy due to changes in lighting conditions and dirt on the cattle's body surface. By using 3D cameras, we aim to build a more stable individual identification system that is less affected by these environmental factors.",environmentTitle:"Experimental Environment",environmentText1:"We conducted experiments using data collected from Miyazaki University's Sumiyoshi Field under agricultural-engineering collaboration. As shown in the figure above, we installed a 3D camera at the top of the cattle barn to capture the cows from above. This made it possible to capture three-dimensional features such as the shape and height of the cow's back.",environmentText2:"The depth information obtained from the 3D camera is saved as a CSV file and utilized in subsequent processing. This method enables stable data collection that is not affected by lighting conditions or the cow's coat color.",methodTitle:"Proposed Method",methodText1:"The flowchart for individual identification is shown. We detect the cow region from the CSV file containing distance information, extract features from the obtained cow region, and perform individual identification using machine learning.",methodText2:"The specific processing flow is as follows:",methodList:["Preprocessing of depth information obtained from 3D camera (noise removal, normalization)","Separation of background and cow regions (threshold processing, region segmentation)","Extraction of features related to the cow's 3D shape (back line, shoulder height, hip shape, etc.)","Construction of individual identification model using machine learning algorithms","Implementation of real-time identification system"],resultsTitle:"Experimental Results",resultsText1:"The results compare the accuracy of individual identification using four machine learning classifiers (A-D). Classifier D achieved an accuracy rate of 95.0%, confirming that high-precision individual identification is possible.",resultsText2:"In particular, it showed stable identification performance that is less affected by environmental factors such as changes in lighting conditions and dirt on the cow's body surface. It was also confirmed that high-accuracy identification is possible even for walking cows, making it a highly practical system.",futureTitle:"Future Prospects",futureText1:"Based on the results of this research, we will continue to expand our dataset and improve our models to accommodate a wider range of environments and cattle breeds. In addition to individual identification, we also aim to develop a comprehensive cattle health management system that integrates other health indicators such as lameness detection through gait pattern analysis and BCS evaluation through body shape tracking.",futureText2:"Furthermore, we are considering building a real-time processing system utilizing edge computing technology and developing an integrated management system for large-scale farms through integration with cloud systems. Through these efforts, we aim to contribute to improving productivity and sustainable development of Japan's dairy industry.",futureText3:"This research aims to contribute to SDG Goal 2 (Zero Hunger), Goal 8 (Decent Work and Economic Growth), and Goal 9 (Industry, Innovation and Infrastructure)."}};var d=i(3999);function u(){let{language:e}=(0,l.o)(),t=c[e];return(0,n.jsxs)("div",{children:[(0,n.jsxs)("section",{className:"relative h-96 bg-cover bg-center bg-no-repeat",style:(0,d.Z6)("/images/normal_header.png"),children:[(0,n.jsx)("div",{className:"absolute inset-0 bg-black/30"}),(0,n.jsx)("div",{className:"container relative z-10",children:(0,n.jsx)("div",{className:"text-center",children:(0,n.jsx)("h1",{className:"text-2xl md:text-3xl lg:text-4xl font-bold leading-tight tracking-tighter text-white drop-shadow-lg",children:t.title})})})]}),(0,n.jsx)("section",{className:"py-16",children:(0,n.jsx)("div",{className:"container",children:(0,n.jsxs)("div",{className:"max-w-4xl mx-auto",children:[(0,n.jsxs)("div",{className:"mb-16",children:[(0,n.jsx)("h2",{className:"text-2xl font-bold mb-4",children:t.overviewTitle}),(0,n.jsx)("p",{className:"text-gray-700 mb-4",children:t.overviewText})]}),(0,n.jsxs)("div",{className:"mb-16",children:[(0,n.jsx)("h2",{className:"text-2xl font-bold mb-4",children:t.backgroundTitle}),(0,n.jsx)("p",{className:"text-gray-700",children:t.backgroundText1}),(0,n.jsx)("p",{className:"text-gray-700 mt-4",children:t.backgroundText2}),(0,n.jsx)("div",{className:"relative aspect-auto mt-8 rounded-lg overflow-hidden",style:{height:"auto",minHeight:"250px"},children:(0,n.jsx)(a.default,{src:(0,d.Dw)("/images/research-siihara1.png"),alt:"ja"===e?"実験背景・目的":"Experimental Background & Objectives",fill:!0,className:"object-contain"})})]}),(0,n.jsxs)("div",{className:"mb-16",children:[(0,n.jsx)("h2",{className:"text-2xl font-bold mb-4",children:t.environmentTitle}),(0,n.jsx)("p",{className:"text-gray-700",children:t.environmentText1}),(0,n.jsx)("p",{className:"text-gray-700 mt-4",children:t.environmentText2}),(0,n.jsx)("div",{className:"relative aspect-auto mt-8 rounded-lg overflow-hidden",style:{height:"auto",minHeight:"250px"},children:(0,n.jsx)(a.default,{src:(0,d.Dw)("/images/research-siihara2.png"),alt:"ja"===e?"実験環境":"Experimental Environment",fill:!0,className:"object-contain"})})]}),(0,n.jsxs)("div",{className:"mb-16",children:[(0,n.jsx)("h2",{className:"text-2xl font-bold mb-4",children:t.methodTitle}),(0,n.jsx)("p",{className:"text-gray-700",children:t.methodText1}),(0,n.jsx)("p",{className:"text-gray-700 mt-4",children:t.methodText2}),(0,n.jsx)("ol",{className:"list-decimal pl-5 space-y-2 mt-2 text-gray-700",children:t.methodList.map((e,t)=>(0,n.jsx)("li",{children:e},t))}),(0,n.jsx)("div",{className:"relative aspect-auto mt-8 rounded-lg overflow-hidden",style:{height:"auto",minHeight:"250px"},children:(0,n.jsx)(a.default,{src:(0,d.Dw)("/images/research-siihara3.png"),alt:"ja"===e?"提案手法":"Proposed Method",fill:!0,className:"object-contain"})})]}),(0,n.jsxs)("div",{className:"mb-16",children:[(0,n.jsx)("h2",{className:"text-2xl font-bold mb-4",children:t.resultsTitle}),(0,n.jsx)("p",{className:"text-gray-700",children:t.resultsText1}),(0,n.jsx)("p",{className:"text-gray-700 mt-4",children:t.resultsText2}),(0,n.jsx)("div",{className:"relative aspect-auto mt-8 rounded-lg overflow-hidden",style:{height:"auto",minHeight:"250px"},children:(0,n.jsx)(a.default,{src:(0,d.Dw)("/images/research-siihara4.png"),alt:"ja"===e?"実験結果":"Experimental Results",fill:!0,className:"object-contain"})})]}),(0,n.jsxs)("div",{className:"mb-16",children:[(0,n.jsx)("h2",{className:"text-2xl font-bold mb-4",children:t.futureTitle}),(0,n.jsx)("p",{className:"text-gray-700",children:t.futureText1}),(0,n.jsx)("p",{className:"text-gray-700 mt-4",children:t.futureText2}),(0,n.jsx)("p",{className:"text-gray-700 mt-4",children:t.futureText3})]}),(0,n.jsx)("div",{className:"flex justify-center mt-8",children:(0,n.jsx)(s(),{href:"/research/projects",children:(0,n.jsx)(o.$,{variant:"outline",children:t.backToProjects})})})]})})})]})}},1469:(e,t,i)=>{"use strict";Object.defineProperty(t,"__esModule",{value:!0}),!function(e,t){for(var i in t)Object.defineProperty(e,i,{enumerable:!0,get:t[i]})}(t,{default:function(){return l},getImageProps:function(){return o}});let n=i(8229),a=i(8883),r=i(3063),s=n._(i(1193));function o(e){let{props:t}=(0,a.getImgProps)(e,{defaultLoader:s.default,imgConf:{deviceSizes:[640,750,828,1080,1200,1920,2048,3840],imageSizes:[16,32,48,64,96,128,256,384],path:"/imagelab/_next/image/",loader:"default",dangerouslyAllowSVG:!1,unoptimized:!0}});for(let[e,i]of Object.entries(t))void 0===i&&delete t[e];return{props:t}}let l=r.Image},2085:(e,t,i)=>{"use strict";i.d(t,{F:()=>s});var n=i(2596);let a=e=>"boolean"==typeof e?`${e}`:0===e?"0":e,r=n.$,s=(e,t)=>i=>{var n;if((null==t?void 0:t.variants)==null)return r(e,null==i?void 0:i.class,null==i?void 0:i.className);let{variants:s,defaultVariants:o}=t,l=Object.keys(s).map(e=>{let t=null==i?void 0:i[e],n=null==o?void 0:o[e];if(null===t)return null;let r=a(t)||a(n);return s[e][r]}),c=i&&Object.entries(i).reduce((e,t)=>{let[i,n]=t;return void 0===n||(e[i]=n),e},{});return r(e,l,null==t||null==(n=t.compoundVariants)?void 0:n.reduce((e,t)=>{let{class:i,className:n,...a}=t;return Object.entries(a).every(e=>{let[t,i]=e;return Array.isArray(i)?i.includes({...o,...c}[t]):({...o,...c})[t]===i})?[...e,i,n]:e},[]),null==i?void 0:i.class,null==i?void 0:i.className)}},3059:(e,t,i)=>{"use strict";i.d(t,{LanguageProvider:()=>s,o:()=>o});var n=i(5155),a=i(2115);let r=(0,a.createContext)(void 0);function s(e){let{children:t}=e,[i,s]=(0,a.useState)("ja");return(0,a.useEffect)(()=>{let e=localStorage.getItem("language");e&&("ja"===e||"en"===e)&&s(e)},[]),(0,n.jsx)(r.Provider,{value:{language:i,setLanguage:e=>{s(e),localStorage.setItem("language",e)}},children:t})}function o(){let e=(0,a.useContext)(r);if(void 0===e)throw Error("useLanguage must be used within a LanguageProvider");return e}},3999:(e,t,i)=>{"use strict";i.d(t,{Dw:()=>s,Z6:()=>l,cn:()=>r,iA:()=>o});var n=i(2596),a=i(9688);function r(){for(var e=arguments.length,t=Array(e),i=0;i<e;i++)t[i]=arguments[i];return(0,a.QP)((0,n.$)(t))}function s(e){e.startsWith("./")&&(e=e.replace("./","/"));{var t,i;let n=null==(i=document.querySelector('script[src*="/_next/"]'))||null==(t=i.getAttribute("src"))?void 0:t.match(/^(\/[^\/]+)?\//);if(n&&n[1])return n[1]+e}return e}function o(e){{var t,i;let n=null==(i=document.querySelector('script[src*="/_next/"]'))||null==(t=i.getAttribute("src"))?void 0:t.match(/^(\/[^\/]+)?\//);if(n&&n[1])return n[1]+e}return e}function l(e){return{backgroundImage:"url('".concat(s(e),"')")}}},6101:(e,t,i)=>{"use strict";i.d(t,{s:()=>s,t:()=>r});var n=i(2115);function a(e,t){if("function"==typeof e)return e(t);null!=e&&(e.current=t)}function r(...e){return t=>{let i=!1,n=e.map(e=>{let n=a(e,t);return i||"function"!=typeof n||(i=!0),n});if(i)return()=>{for(let t=0;t<n.length;t++){let i=n[t];"function"==typeof i?i():a(e[t],null)}}}}function s(...e){return n.useCallback(r(...e),e)}},6766:(e,t,i)=>{"use strict";i.d(t,{default:()=>a.a});var n=i(1469),a=i.n(n)},7168:(e,t,i)=>{"use strict";i.d(t,{$:()=>c});var n=i(5155),a=i(2115),r=i(9708),s=i(2085),o=i(3999);let l=(0,s.F)("inline-flex items-center justify-center gap-2 whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 [&_svg]:pointer-events-none [&_svg]:size-4 [&_svg]:shrink-0",{variants:{variant:{default:"bg-primary text-primary-foreground hover:bg-primary/90",destructive:"bg-destructive text-destructive-foreground hover:bg-destructive/90",outline:"border border-input bg-background hover:bg-accent hover:text-accent-foreground",secondary:"bg-secondary text-secondary-foreground hover:bg-secondary/80",ghost:"hover:bg-accent hover:text-accent-foreground",link:"text-primary underline-offset-4 hover:underline"},size:{default:"h-10 px-4 py-2",sm:"h-9 rounded-md px-3",lg:"h-11 rounded-md px-8",icon:"h-10 w-10"}},defaultVariants:{variant:"default",size:"default"}}),c=a.forwardRef((e,t)=>{let{className:i,variant:a,size:s,asChild:c=!1,...d}=e,u=c?r.Slot:"button";return(0,n.jsx)(u,{className:(0,o.cn)(l({variant:a,size:s,className:i})),ref:t,...d})});c.displayName="Button"},7744:(e,t,i)=>{Promise.resolve().then(i.bind(i,546))},9708:(e,t,i)=>{"use strict";i.d(t,{Slot:()=>o,TL:()=>s});var n=i(2115),a=i(6101),r=i(5155);function s(e){let t=function(e){let t=n.forwardRef((e,t)=>{var i,r,s;let o,l,{children:c,...d}=e,u=n.isValidElement(c)?(l=(o=null==(r=Object.getOwnPropertyDescriptor((i=c).props,"ref"))?void 0:r.get)&&"isReactWarning"in o&&o.isReactWarning)?i.ref:(l=(o=null==(s=Object.getOwnPropertyDescriptor(i,"ref"))?void 0:s.get)&&"isReactWarning"in o&&o.isReactWarning)?i.props.ref:i.props.ref||i.ref:void 0,m=(0,a.s)(u,t);if(n.isValidElement(c)){let e=function(e,t){let i={...t};for(let n in t){let a=e[n],r=t[n];/^on[A-Z]/.test(n)?a&&r?i[n]=function(){for(var e=arguments.length,t=Array(e),i=0;i<e;i++)t[i]=arguments[i];let n=r(...t);return a(...t),n}:a&&(i[n]=a):"style"===n?i[n]={...a,...r}:"className"===n&&(i[n]=[a,r].filter(Boolean).join(" "))}return{...e,...i}}(d,c.props);return c.type!==n.Fragment&&(e.ref=m),n.cloneElement(c,e)}return n.Children.count(c)>1?n.Children.only(null):null});return t.displayName="".concat(e,".SlotClone"),t}(e),i=n.forwardRef((e,i)=>{let{children:a,...s}=e,o=n.Children.toArray(a),l=o.find(c);if(l){let e=l.props.children,a=o.map(t=>t!==l?t:n.Children.count(e)>1?n.Children.only(null):n.isValidElement(e)?e.props.children:null);return(0,r.jsx)(t,{...s,ref:i,children:n.isValidElement(e)?n.cloneElement(e,void 0,a):null})}return(0,r.jsx)(t,{...s,ref:i,children:a})});return i.displayName="".concat(e,".Slot"),i}var o=s("Slot"),l=Symbol("radix.slottable");function c(e){return n.isValidElement(e)&&"function"==typeof e.type&&"__radixId"in e.type&&e.type.__radixId===l}}},e=>{var t=t=>e(e.s=t);e.O(0,[6874,3063,4277,8441,1684,7358],()=>t(7744)),_N_E=e.O()}]);